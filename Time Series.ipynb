{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Time series analysis\n",
    "\n",
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take real time-series data of total ads watched by hour in a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/ads_hour.csv',index_col=['Date'], parse_dates=['Date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('bmh'):    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.title('Ads watched (hour ticks)')\n",
    "    plt.plot(df.ads);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything you could ask for - trend, seasonality, even some outliers.\n",
    "\n",
    "In this notebook we will concentrate on methods that have proven to be working in practice and can provide quality, comparable to ARIMA models. Namely, feature engineering, selecting and machine learning\n",
    "\n",
    "- First possible solution - let's get dummies and make 24 new columns out of one. Clear disadvantages - we explode the dimentionality of our data and lose any trace of the cyclical nature of hours.\n",
    "\n",
    "- Second solution - sine/cosine transformation. To fully understand that approach, read [this short article](https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/). But in short - we want to encode hour feature with two new columns, which are sine and cosine transformations of \"hour from midnight\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the already familiar `prepareData` function with some modifications - sine/cosine transformation for `hour` and dummy transformation for `weekday` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def prepareData(data, lag_start=5, lag_end=14, test_size=0.15):\n",
    "    \"\"\"\n",
    "    series: pd.DataFrame\n",
    "        dataframe with timeseries\n",
    "\n",
    "    lag_start: int\n",
    "        initial step back in time to slice target variable \n",
    "        example - lag_start = 1 means that the model \n",
    "                  will see yesterday's values to predict today\n",
    "\n",
    "    lag_end: int\n",
    "        final step back in time to slice target variable\n",
    "        example - lag_end = 4 means that the model \n",
    "                  will see up to 4 days back in time to predict today\n",
    "\n",
    "    test_size: float\n",
    "        size of the test dataset after train/test split as percentage of dataset\n",
    "\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame(data.copy())\n",
    "    data.columns = [\"y\"]\n",
    "    \n",
    "    # Step 1: calculate test index start position to split data on train test\n",
    "    test_index = int(len(data) * (1 - test_size))\n",
    "    \n",
    "    # Step 2: adding lags of original time series data as features\n",
    "    for i in range(lag_start, lag_end):\n",
    "        data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "        \n",
    "    # Step 3: transforming df index to datetime and creating new variables\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "        \n",
    "    # since we will be using only linear models we need to get dummies from weekdays \n",
    "    # to avoid imposing weird algebraic rules on day numbers\n",
    "    data = pd.concat([\n",
    "        data.drop(\"weekday\", axis=1), \n",
    "        pd.get_dummies(data['weekday'], prefix='weekday')\n",
    "    ], axis=1)\n",
    "        \n",
    "    # Step 4: encode hour with sin/cos transformation\n",
    "    # credits - https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/\n",
    "    data['sin_hour'] = np.sin(2*np.pi*data['hour']/24)\n",
    "    data['cos_hour'] = np.cos(2*np.pi*data['hour']/24)\n",
    "    data.drop([\"hour\"], axis=1, inplace=True)\n",
    "        \n",
    "\n",
    "    data = data.dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Step 5: splitting whole dataset on train and test\n",
    "    X_train = data.loc[:test_index].drop([\"y\"], axis=1)\n",
    "    y_train = data.loc[:test_index][\"y\"]\n",
    "    X_test = data.loc[test_index:].drop([\"y\"], axis=1)\n",
    "    y_test = data.loc[test_index:][\"y\"]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the functions on original `df` to prepare datasets necessary for model training. Reserve 30% of data for testing, using initial lag 12 and final lag 48. This way the model will be able to make forecasts twelve steps ahead, having observed data from the previous 1.5 day. \n",
    "\n",
    "We'll scale the resulting datasets with the help of `StandardScaler` and create new variables - `X_train_scaled` and `X_test_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepareData(df, lag_start=12, lag_end=48, test_size=0.3)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_train)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train a simple linear regression on scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "simple = regr.fit(X_train_scaled, X_test_scaled)\n",
    "simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll check the quality of the model on the training set via cross-validation. To do so we'll create an  object-generator of time series cv folds with the help of  `TimeSeriesSplit`. Set the number of folds to be equal to 5. Then make use of `cross_val_score`, feeding it's `cv` parameter with the created generator. Quality metrics should be `neg_mean_absolute_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "a = cross_val_score(regr, X_train_scaled, y_train, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "b = np.mean(a)\n",
    "b\n",
    "c = b * -1\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The value of MAE on cross-validation is 4490**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the forecast itself. We'll use `plotModelResults` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plotModelResults(model, df_train, df_test, y_train, y_test, plot_intervals=False, scale=1.96, cv=tscv):\n",
    "    \"\"\"\n",
    "    Plots modelled vs fact values\n",
    "    \n",
    "    model: fitted model \n",
    "    \n",
    "    df_train, df_test: splitted featuresets\n",
    "    \n",
    "    y_train, y_test: targets\n",
    "    \n",
    "    plot_intervals: bool, if True, plot prediction intervals\n",
    "    \n",
    "    scale: float, sets the width of the intervals\n",
    "    \n",
    "    cv: cross validation method, needed for intervals\n",
    "    \n",
    "    \"\"\"\n",
    "    # Step 1: making predictions for test\n",
    "    prediction = model.predict(df_test)\n",
    "    \n",
    "    plt.figure(figsize=(20, 7))\n",
    "    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
    "    \n",
    "    if plot_intervals:\n",
    "        # Step 2: calculate cv scores\n",
    "        cv = cross_val_score(\n",
    "            model, \n",
    "            df_train, \n",
    "            y_train, \n",
    "            cv=cv, \n",
    "            scoring=\"neg_mean_squared_error\"\n",
    "        )\n",
    "\n",
    "        # Step 3: calculate cv error deviation\n",
    "        deviation = np.sqrt(cv.std())\n",
    "        \n",
    "        # Step 4: calculate lower and upper intervals\n",
    "        lower = prediction - (scale * deviation)\n",
    "        upper = prediction + (scale * deviation)\n",
    "        \n",
    "        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
    "        plt.plot(upper, \"r--\", alpha=0.5)\n",
    "        \n",
    "    # Step 5: calculate overall quality on test set\n",
    "    mae  = mean_absolute_error(prediction, y_test)\n",
    "    mape = mean_absolute_percentage_error(prediction, y_test)\n",
    "    plt.title(\"MAE {}, MAPE {}%\".format(round(mae), round(mape, 2)))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model coefficients visualization we'll use the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCoefficients(model):\n",
    "    \"\"\"Returns sorted coefficient values of the model\"\"\"\n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    return coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)    \n",
    "    \n",
    "\n",
    "def plotCoefficients(model):\n",
    "    \"\"\"Plots sorted coefficient values of the model\"\"\"\n",
    "    coefs = getCoefficients(model)\n",
    "    \n",
    "    plt.figure(figsize=(20, 7))\n",
    "    coefs.coef.plot(kind='bar')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll train Lasso regression on cross-validation (`LassoCV`) again feeding the `cv` parameter with the created generator-object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV()\n",
    "simple1 = lasso.fit(X_train_scaled, y_train)\n",
    "simple1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "a1 = cross_val_score(lasso, X_train_scaled, y_train, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "b1 = np.mean(a1)\n",
    "b1\n",
    "c1 = b1 * -1\n",
    "c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, we are still having practically the same model quality, while having less features. Using the function `getCoefficients` we'll find out how many features have been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getCoefficients(lasso).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we have some features dropped. But what if we want to go further and transform our linear-dependant features into more compact representation? To do so we'll use principal component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def plotPCA(pca):\n",
    "    \"\"\"\n",
    "    Plots accumulated percentage of explained variance by component\n",
    "    \n",
    "    pca: fitted PCA object\n",
    "    \"\"\"\n",
    "    components = range(1, pca.n_components_ + 1)\n",
    "    variance = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(components, variance)\n",
    "    \n",
    "    # additionally mark the level of 95% of explained variance \n",
    "    plt.hlines(y = 95, xmin=0, xmax=len(components), linestyles='dashed', colors='red')\n",
    "    \n",
    "    plt.xlabel('PCA components')\n",
    "    plt.ylabel('variance')\n",
    "    plt.xticks(components)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create PCA object: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Step 2: Train PCA on scaled data\n",
    "simple2 = pca.fit(X_train_scaled, y_train)\n",
    "simple2\n",
    "\n",
    "# Step 3: plot explained variance\n",
    "plotPCA(simple2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can observe that the minimum number of components needed to explain at least 95% of variance of the train dataset, is 9 components.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create the `pca` object once again, this time setting inside it the optimal number of components (explaining at least 95% of variance). After that we'll create two new variables - `pca_features_train` and `pca_features_test`, assigning to them pca-transformed scaled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA object: pca\n",
    "pca = PCA(n_components=9)\n",
    "\n",
    "pca_features_train = pca.fit_transform(X_train_scaled) \n",
    "pca_features_test = pca.fit_transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train linear regression on pca features and plot its forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "simple3 = regr.fit(pca_features_train, y_train)\n",
    "simple3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "a2 = cross_val_score(regr, pca_features_train, y_train, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "b2 = np.mean(a2)\n",
    "b2\n",
    "c2 = b2 * -1\n",
    "c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now with the model trained on pca-transformed features, the MAE of linear regression is 4663**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
