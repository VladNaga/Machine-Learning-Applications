{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Principal Component Analysis and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we are going to walk through `sklearn` built-in implementations of dimensionality reduction and clustering methods.\n",
    "## 1. Principal Component Analysis\n",
    "\n",
    "First we'll import all required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns; sns.set(style='white')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the given toy data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[2., 13.], [1., 3.], [6., 19.],\n",
    "              [7., 18.], [5., 17.], [4., 9.],\n",
    "              [5., 22.], [6., 11.], [8., 25.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:, 1])\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With the following code we will plot $x_1$ axis and the vector corresponding to the first principal component for this data, while rescaling the data using StandardScaler.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(X)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(scaled_data[:,0], X[:, 1])\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [i*scaled_data[0] for i in range(-3,4,1)]\n",
    "x2 = [i*scaled_data[1] for i in range(-3,4,1)]\n",
    "plt.plot(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **With the following code we will find what are the eigenvalues of the $X^{\\text{T}}X$ matrix, given $X$ is a rescaled matrix of the toy dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.dot(X.T, X)\n",
    "c = np.linalg.eigvals(z)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a dataset of peoples' faces and output their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=50, \n",
    "                resize=0.4, data_home='../../data/faces')\n",
    "\n",
    "print('%d objects, %d features, %d classes' % (lfw_people.data.shape[0],\n",
    "      lfw_people.data.shape[1], len(lfw_people.target_names)))\n",
    "print('\\nPersons:')\n",
    "for name in lfw_people.target_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some faces. All images are stored in a handy `lfw_people.images` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(15):\n",
    "    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(lfw_people.images[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With the following code we will find what the minimal principal component number is needed to explain 90% of data variance.**\n",
    "\n",
    "For this task, we will be using the [`svd_solver='randomized'`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) parameter, which is a PCA approximation, but it significantly increases performance on large data sets. We'll also use fixed `random_state=1` for comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.90, random_state =1)\n",
    "pca.fit(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "svd_solver = 'randomized'\n",
    "imgs=lfw_people.images.reshape(1560,50*37)\n",
    "#pca = StandardScaler('imgs').fit_transform()\n",
    "pca = PCA(n_components = 0.90, random_state =1)\n",
    "pca.fit(imgs)\n",
    "#print(pca.explained_variance_)\n",
    "X_pca = pca.transform(imgs)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a picture showing the first 30 principal components. In order to create it, use 30 vectors from `pca.components_`, reshape them to their initial size (50 x 37), and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next task in the notebook, we'll load the housing prices dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "X = boston.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the elbow-method (reference [article 7](https://medium.com/@libfun/db7879568417) of the course), we'll find the optimal number of clusters to set as a hyperparameter for the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With the following code we will find what is the optimal number of clusters to use on housing prices data set according to the elbow-method.**\n",
    "\n",
    "In this case, we are looking for the most significant curve fracture on the `Cluster number vs Centroid distances` graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters= k, random_state=1).fit(X)\n",
    "    inertia.append(np.sqrt(kmeans.inertia_))\n",
    "inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 10), inertia, marker='s');\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('$J(C_k)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the faces dataset. We can imagine that we did not know the names for who was each photo but that we knew that there were 12 different people. Let's compare clustering results from 4 algorithms - k-means, Agglomerative clustering, Affinity Propagation, and Spectral clustering. We'll use the same respective parameters as in the end of [this article](https://medium.com/@libfun/db7879568417), only change the number of clusters to 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "kmeanss = KMeans(n_clusters=12, random_state=1).fit(imgs)\n",
    "kmeanss.labels_\n",
    "\n",
    "#kmeanss.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "clustering = AgglomerativeClustering(n_clusters = 12).fit(imgs)\n",
    "clustering \n",
    "# AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
    "#             connectivity=None, linkage='ward', memory=None, n_clusters=2,\n",
    "#             pooling_func='deprecated')\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "import numpy as np\n",
    "clusteringg = AffinityPropagation().fit(imgs)\n",
    "clusteringg \n",
    "#AffinityPropagation(affinity='euclidean', convergence_iter=15, copy=True,\n",
    "#         damping=0.5, max_iter=200, preference=None, verbose=False)\n",
    "clusteringg.labels_\n",
    "#clusteringg.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "import numpy as np\n",
    "clustering = SpectralClustering(n_clusters=12, assign_labels=\"discretize\", random_state=1).fit(imgs)\n",
    "clustering.labels_\n",
    "# clustering \n",
    "# SpectralClustering(affinity='rbf', assign_labels='discretize', coef0=1,\n",
    "#           degree=3, eigen_solver=None, eigen_tol=0.0, gamma=1.0,\n",
    "#           kernel_params=None, n_clusters=2, n_init=10, n_jobs=None,\n",
    "#           n_neighbors=10, random_state=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
