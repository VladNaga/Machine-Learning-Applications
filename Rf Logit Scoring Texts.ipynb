{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Here we will develop and tune models for credit scoring and movies reviews sentiment prediction. (https://docs.google.com/forms/d/1MS3kW_bjZQAkwwlAjX9G8khj1owq1qc5NQtjzJUvKVo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The [dataset](https://github.com/Yorko/mlcourse.ai/tree/master/data/credit_scoring_sample.csv) looks like this:\n",
    "\n",
    "##### Target variable\n",
    "* SeriousDlqin2yrs - the person had long delays in payments during 2 years; binary variable\n",
    "\n",
    "##### Features\n",
    "* age - Age of the loan borrower (number of full years); type - integer\n",
    "* NumberOfTime30-59DaysPastDueNotWorse - the number of times a person has had a delay in repaying other loans more than 30-59 days (but not more) during last two years; type - integer\n",
    "* DebtRatio - monthly payments (loans, alimony, etc.) divided by aggregate monthly income, percentage; float type\n",
    "* MonthlyIncome - monthly income in dollars; float type\n",
    "* NumberOfTimes90DaysLate - the number of times a person has had a delay in repaying other loans for more than 90 days; type - integer\n",
    "* NumberOfTime60-89DaysPastDueNotWorse - the number of times a person has had a delay in repaying other loans more than 60-89 days (but not more) in the last two years; type - integer\n",
    "* NumberOfDependents - number of people in the family of the borrower; type - integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement a function that will replace the NaN values by the median in each column of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan_with_median(table):\n",
    "    for col in table.columns:\n",
    "        table[col]= table[col].fillna(table[col].median())\n",
    "    return table   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/credit_scoring_sample.csv', sep=\";\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View data types of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distribution of classes in target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data['SeriousDlqin2yrs'].hist(orientation='horizontal', color='red')\n",
    "ax.set_xlabel(\"number_of_observations\")\n",
    "ax.set_ylabel(\"unique_value\")\n",
    "ax.set_title(\"Target distribution\")\n",
    "\n",
    "print('Distribution of target:')\n",
    "data['SeriousDlqin2yrs'].value_counts() / data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll select all the features and drop the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_columns_names = data.columns.values\n",
    "independent_columns_names = [x for x in data if x != 'SeriousDlqin2yrs']\n",
    "independent_columns_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a function that replaces all values of NaN by the median value of the corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = impute_nan_with_median(data)\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overdue = table[table['NumberOfTime30-59DaysPastDueNotWorse'] + table['NumberOfTimes90DaysLate']\n",
    "                + table['NumberOfTime60-89DaysPastDueNotWorse'] > 0 ]\n",
    "overdue = np.array(overdue['MonthlyIncome'])\n",
    "overdue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_time = table[table['NumberOfTime30-59DaysPastDueNotWorse'] + table['NumberOfTimes90DaysLate']\n",
    "                + table['NumberOfTime60-89DaysPastDueNotWorse'] == 0 ]\n",
    "on_time = np.array(on_time['MonthlyIncome'])\n",
    "on_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the target and features - now we get a training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = table[independent_columns_names]\n",
    "y = table['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make an interval estimate based on the bootstrap of the average income (MonthlyIncome)  of customers who had overdue loan payments, and of those who paid in time, make 90% confidence interval. We'll also find the difference between the lower limit of the derived interval for those who paid in time and the upper limit for those who are overdue.\n",
    "\n",
    "We'll use the example from the [article](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-5-ensembles-of-algorithms-and-random-forest-8e05246cbba7). Set `np.random.seed (17)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_samples(data, n_samples, seed=0):\n",
    "    # Function to generate subsamples with bootstrap\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.randint(0, len(data), (n_samples, len(data)))\n",
    "    samples = data[indices]\n",
    "    return samples\n",
    "\n",
    "def stat_intervals(stat, alpha):\n",
    "    # Function for interval estimates\n",
    "    boundaries = np.percentile(stat, [100 * alpha / 2., 100 * (1 - alpha / 2.)])\n",
    "    return boundaries\n",
    "\n",
    "# Save data about overdues in different numpy arrays\n",
    "churn = data[data['SeriousDlqin2yrs'] == 1]['MonthlyIncome'].values\n",
    "not_churn = data[data['SeriousDlqin2yrs'] == 0]['MonthlyIncome'].values\n",
    "\n",
    "# Generate bootstrap samples and calculate the means\n",
    "churn_mean_scores = [np.mean(sample) \n",
    "                     for sample in get_bootstrap_samples(churn, 1000, seed=17)]\n",
    "not_churn_mean_scores = [np.mean(sample) \n",
    "                         for sample in get_bootstrap_samples(not_churn, 1000, seed=17)]\n",
    "\n",
    "#  Derive interval estimate of the mean\n",
    "print(\"Mean interval\",  stat_intervals(churn_mean_scores, 0.1))\n",
    "print(\"Mean interval\",  stat_intervals(not_churn_mean_scores, 0.1))\n",
    "print(\"Difference is\", stat_intervals(not_churn_mean_scores, 0.1)[0] - \n",
    "      stat_intervals(churn_mean_scores, 0.1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree, hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main performance metrics of a model is the area under the ROC curve. The ROC-AUC values lay between 0 and 1. The closer the value of ROC-AUC to 1, the better the classification is done.\n",
    "\n",
    "We'll find the values of `DecisionTreeClassifier` hyperparameters using the `GridSearchCV`, which maximize the area under the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `DecisionTreeClassifier` class to create a decision tree. Due to the imbalance of the classes in the target, we add the balancing parameter. We also use the parameter `random_state = 17` for the reproducibility of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=17, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look through such values of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_values = [5, 6, 7, 8, 9]\n",
    "max_features_values = [4, 5, 6, 7]\n",
    "tree_params = {'max_depth': max_depth_values,\n",
    "               'max_features': max_features_values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix cross-validation parameters: stratified, 5 partitions with shuffle, \n",
    "`random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll run GridSearch with the ROC AUC metric using the hyperparameters from the `tree_params` dictionary. We'll find what is the maximum ROC AUC value. We call cross-validation stable if the standard deviation of the metric on the cross-validation is less than 1%.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid_search = GridSearchCV(dt, tree_params, n_jobs=-1, scoring ='roc_auc', cv=skf)\n",
    "dt_grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(float(dt_grid_search.best_score_), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid_search.cv_results_[\"std_test_score\"][np.argmax(dt_grid_search.cv_results_[\"mean_test_score\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RandomForest implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Task 4.</font>**\n",
    "We'll implement our own random forest using `DecisionTreeClassifier` with the best parameters from the previous task.\n",
    "\n",
    "Brief specification:\n",
    " - In the `fit` method in the loop (`i` from 0 to `n_estimators-1`), fix the seed equal to (`random_state + i`). The idea is that at each iteration there's a new value of random seed to add more \"randomness\", but at hte same time results are reproducible\n",
    " - After fixing the seed, select `max_features` features **without replacement**, save the list of selected feature ids in `self.feat_ids_by_tree`\n",
    " - Also make a bootstrap sample (i.e. **sampling with replacement**) of training instances. For that, resort to `np.random.choice` and its argument `replace`\n",
    " - Train a decision tree with specified (in a constructor) arguments `max_depth`, `max_features` and `random_state` (do not specify `class_weight`) on a corresponding subset of training data. \n",
    " - The `fit` method returns the current instance of the class `RandomForestClassifierCustom`, that is `self`\n",
    " - In the `predict_proba` method, we need to loop through all the trees. For each prediction, obviously, we need to take only those features which we used for training the corresponding tree. The method returns predicted probabilities (`predict_proba`), averaged for all trees\n",
    "\n",
    "We'll perform cross-validation, and find what is the average ROC AUC for cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class RandomForestClassifierCustom(BaseEstimator):\n",
    "    def __init__(self, n_estimators=10, max_depth=10, max_features=10, \n",
    "                 random_state=17):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.trees = []\n",
    "        self.feat_ids_by_tree = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            \n",
    "            np.random.seed(i + self.random_state)\n",
    "            \n",
    "            feat_to_use_ids = np.random.choice(range(X.shape[1]), self.max_features, \n",
    "                                              replace=False)\n",
    "            examples_to_use = list(set(np.random.choice(range(X.shape[0]), X.shape[0],\n",
    "                                              replace=True)))\n",
    "            \n",
    "            self.feat_ids_by_tree.append(feat_to_use_ids)\n",
    "            \n",
    "            dt = DecisionTreeClassifier(\n",
    "                                        max_depth=self.max_depth, \n",
    "                                        max_features=self.max_features, \n",
    "                                        random_state = self.random_state)\n",
    "\n",
    "            dt.fit(X[examples_to_use, :][:, feat_to_use_ids], y[examples_to_use])\n",
    "            self.trees.append(dt)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        predictions = []\n",
    "        for i in range(self.n_estimators):\n",
    "            feat_to_use_ids = self.feat_ids_by_tree[i]\n",
    "            predictions.append(self.trees[i].predict_proba(X[:,feat_to_use_ids]))\n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifierCustom(max_depth=7, max_features=6).fit(X.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv_aucs = cross_val_score(RandomForestClassifierCustom(max_depth=7, max_features=6), \n",
    "                          X.values, y.values, scoring=\"roc_auc\", cv=skf)\n",
    "print(\"Mean ROC AUC:\", np.mean(cv_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare our own implementation of a random forest with `sklearn` version of it. To do this, we'll use `RandomForestClassifier (class_weight='balanced', random_state=17)` and specify all the same values for `max_depth` and `max_features` as before. After that we'll find what the average value of ROC AUC on cross-validation we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_aucs = cross_val_score(RandomForestClassifier(n_estimators=10, max_depth=7, \n",
    "                                               max_features=6,\n",
    "                                               random_state=17, n_jobs=-1,\n",
    "                                              class_weight='balanced'), \n",
    "                        X.values, y.values, scoring=\"roc_auc\", cv=skf)\n",
    "print(\"Mean ROC AUC for sklearn RF:\", np.mean(cv_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `sklearn` RandomForest, hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extend the value of `max_depth` up to 15, because the trees need to be deeper in the forest (more information can be seen from this [article](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-5-ensembles-of-algorithms-and-random-forest-8e05246cbba7)). We'll find out what are the best values of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_values = range(5, 15)\n",
    "max_features_values = [4, 5, 6, 7]\n",
    "forest_params = {'max_depth': max_depth_values,\n",
    "                'max_features': max_features_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(random_state=17, n_jobs=-1, \n",
    "                            class_weight='balanced')\n",
    "rf_grid_search = GridSearchCV(rf, forest_params, n_jobs=-1, \n",
    "                              scoring='roc_auc', cv=skf)\n",
    "rf_grid_search.fit(X.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_search.cv_results_[\"std_test_score\"][np.argmax(rf_grid_search.cv_results_[\"mean_test_score\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression, hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our results with logistic regression (we indicate `class_weight='balanced'` and `random_state = 17`). We'll do a full search by the parameter `C` from a wide range of values `np.logspace(-8, 8, 17)`.\n",
    "Now we will build a pipeline - first apply scaling, then train the model.\n",
    "\n",
    "We'll find what is the best average ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scaler = StandardScaler()\n",
    "logit = LogisticRegression(random_state=17, class_weight='balanced')\n",
    "\n",
    "logit_pipe = Pipeline([('scaler', scaler), ('logit', logit)])\n",
    "logit_pipe_params = {'logit__C': np.logspace(-8, 8, 17)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logit_pipe_grid_search = GridSearchCV(logit_pipe, logit_pipe_params, n_jobs=-1, \n",
    "                           scoring ='roc_auc', cv=skf)\n",
    "logit_pipe_grid_search.fit(X.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pipe_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression and RandomForest on sparse features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of a small number of features, random forest was proved to be better than logistic regression. However, one of the main disadvantages of trees is how they work with sparse data, for example, with texts. Let's compare logistic regression and random forest in a new task.\n",
    "Download dataset with reviews of movies [here](http://d.pr/f/W0HpZh). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "df = pd.read_csv(\"../../data/movie_reviews_train.csv\", nrows=50000)\n",
    "\n",
    "# Split data to train and test\n",
    "X_text = df[\"text\"]\n",
    "y_text = df[\"label\"]\n",
    "\n",
    "# Classes counts\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Split on 3 folds\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
    "\n",
    "# In Pipeline we will modify the text and train logistic regression\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_features=100000, ngram_range=(1, 3))),\n",
    "    ('clf', LogisticRegression(random_state=17))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Logistic Regression: we'll iterate parameter `C` with values from the list [0.1, 1, 10, 100] and find the best ROC AUC in cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters = {'clf__C': (0.1, 1, 10, 100)}\n",
    "grid_search = GridSearchCV(classifier, parameters, n_jobs=-1, scoring ='roc_auc', cv=skf)\n",
    "grid_search = grid_search.fit(X_text, y_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try to perform the same operation with random forest. Similarly, we'll look over all the values and get the maximum ROC AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_features=100000, ngram_range=(1, 3))),\n",
    "    ('clf', RandomForestClassifier(random_state=17, n_jobs=-1))])\n",
    "\n",
    "min_samples_leaf = [1, 2, 3]\n",
    "max_features = [0.3, 0.5, 0.7]\n",
    "max_depth = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters = {'clf__max_features': max_features,\n",
    "              'clf__min_samples_leaf': min_samples_leaf,\n",
    "              'clf__max_depth': max_depth}\n",
    "grid_search = GridSearchCV(classifier, parameters, n_jobs=-1, scoring ='roc_auc', cv=skf)\n",
    "grid_search = grid_search.fit(X_text, y_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
